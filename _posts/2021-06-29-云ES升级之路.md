# 1. 背景

##  1.1 升级背景	   

随着项目的调用量和调用方越来越多，ES接口的调用量已经占到了整体调用量的1/4，每天调用量1亿左右。项目原有ES为自建ES，自建ES版本为5.4.1,  lucene版本为6.5.1。版本较低，5.X版本存在一些查询的BUG，会导致直接查挂ES集群，且lucene性能在不断优化，云ES的稳定性要好于自建ES，想要获得更高的性能和更好的可用性，升级在所难免。

升级需要考虑的几个问题：

* 5.X  -->  7.10.1的版本升级，如何实现对线上毫无感知，不停服？
* 自建ES --> 云ES配置转换，如何选择云ES机器配置
* 云ES和自建ES的性能差距，需要有实际的压测数据。 

## 1.2 自建ES现状

双集群（双机房），每个集群4台机器，6个分片，2个副本。机器型号为32核64G，超大磁盘 4T 普通磁盘 + 400G SSD。虽然有双集群，但随着调用量的提升，单集群无法承载全部流量，而且集群切换需要手动更改配置中心。

数据节点和主节点混部。



# 2. 云ES选型

版本：7.10.1 （云ES当前最高版本）

数据节点：8个，32核64G，500GB SSD

专用主节点：3个，分属在3个机房

双可用区：当某个节点挂掉以后，自动踢掉此节点。当一个可用区不可用的时候，集群依旧有超过半数的法定主节点选举个数，可以保证集群的正常选主。



# 3. 跨版本升级

## 3.1 问题

1. 从ES集群角度来讲， 5.X -> 7.X 版本跨度过大，协议不兼容，无法做滚动升级，势必需要双写。

2. 从自身服务角度来讲，要求4个9的可用性，5.X -> 7.X SDK发生了变化，TCP->HTTP client，也无法直接切换，需要灰度切换。

3. 新旧集群的数据对账问题

   

## 3.2 思路

### 3.2.1 业务架构

简化一下我们的业务架构：

API服务：提供DB和ES的接口供使用方查询，DB的数据变动的时候，通过redis消息通知异步服务
异步服务：从redis接受消息，做一系列的逻辑整合，最终写数据到ES中去

因此API服务主要涉及到了ES的读取，异步服务涉及到了ES的写入。两者都需要做升级



### 3.1.2 核心问题

|   核心问题   | 解决方案                                                     |
| :----------: | ------------------------------------------------------------ |
| 升级平滑过度 | 通过部署双写集群，冗余队列，灰度发布等手段来过渡，详情见下面流程 |
|   数据一致   | DB和ES对账                                                   |
|  ES集群容量  | 新ES集群全面压测，和老ES集群的压测数据对比                   |

![ES升级1](https://fayewu.github.io/assets/img/ES%E5%8D%87%E7%BA%A71.jpeg)



### 3.1.3 高低版本ES差异

|                           7.X版本                            |              5.X版本              |
| :----------------------------------------------------------: | :-------------------------------: |
|                   client升级为HTTP Client                    |        client为TCP Client         |
|                          取消了type                          |           index下有type           |
| total超过10000后不返回具体数字，需要单独设置参数track_total_hits为true |      直接返回具体数字无限制       |
|                       fix了模糊查询BUG                       | 模糊查询有BUG，可能导致ES直接挂掉 |



## 3.2 流程

如上图所示：

1. 往高版本ES集群中刷入历史数据，进行压测。对比高低版本的压测性能差异，观察ES的读写，CPU使用率等各项指标
2. 我们需要单独部署B版本的一套异步服务，B版本的异步服务使用了ES最新的Client elasticsearch-rest-high-level-client,  A版本的异步服务是老版本的TCP client，两个集群分别消费不同的redis队列组
3. 我们需要升级API服务，使每次变动都双写redis队列，每次变动都推送到两个队列中，供A，B版本的异步服务读取。
4. 1，2完成后，每次数据的变动都会同步写到高低版本的ES集群中去了，观察一段时间有无错误
5. 再次往高版本ES集群中刷入历史数据，并进行对账。
6. 灰度发布API服务，逐步放量切换至高版本ES集群的读写



# 4. 收益

## 4.1 性能

**单ID查询：**

|         集群         |  QPS   | 平均耗时 |
| :------------------: | :----: | :------: |
|  云ES集群（高版本）  | 15026  |  52.7ms  |
| 自建ES集群（低版本） | 5200*2 |  1206ms  |

**大批量/批量账号查询**

100账号查询：

|         集群         |  QPS  | 平均耗时 |
| :------------------: | :---: | :------: |
|  云ES集群（高版本）  | 1003  | 69.82ms  |
| 自建ES集群（低版本） | 124*2 |  1786ms  |

1000账号查询：

|         集群         | QPS  | 平均耗时 |
| :------------------: | :--: | :------: |
|  云ES集群（高版本）  | 683  | 218.49ms |
| 自建ES集群（低版本） | 12*2 |  2255ms  |



**深度翻页查询**

云ES集群

| 翻页数量 |  QPS  |
| :------: | :---: |
|   3000   | 99.25 |
|   5000   | 72.35 |
|   7000   | 66.19 |

自建集群

| 翻页数量 |  QPS  |
| :------: | :---: |
|   3000   |  102  |
|   5000   | 51.06 |
|   7000   | 38.25 |



可以看出在正常情况下，云ES的性能远好于自建ES集群，各种场景下提升了50%～几倍，但是深度翻页这种极限场景，由于ES本身的原理和限制，性能没有大的差别，深度翻页还是需要采用其他的解决方案或者从业务上限制。

## 4.2 稳定性

避免了5.X模糊查询拖垮整个ES集群的情况，避免了单集群挂掉要手动切换的情况，从各个角度来看，云的稳定性都好于自建。
